# Fichier: autonomous_optimizer_service.py
# Description: Le cerveau de l'auto-am√©lioration. Analyse les performances et r√©√©crit la strat√©gie de l'IA.

import os
import time
import json
import requests
from prometheus_api_client import PrometheusConnect

# --- CONFIGURATION ---
PROMETHEUS_URL = "http://prometheus-service.monitoring.svc.cluster.local:9090"
STRATEGY_FILE = "model_strategy.json"

# Endpoint de l'API K8s ou d'un service de d√©ploiement pour d√©clencher un Canary
CANARY_DEPLOY_ENDPOINT = "http://deployment-service/deploy-canary" 

# Requ√™tes Prometheus pour analyser la performance
LATENCY_QUERY = 'rate(gemini_duration_seconds_sum{gen_ai_model=~".+"}[5m]) / rate(gemini_duration_seconds_count{gen_ai_model=~".+"}[5m])'
TOKEN_QUERY = 'rate(gemini_token_usage_total{gen_ai_model=~".+"}[5m])'

def analyze_performance(prom):
    """Analyse les m√©triques de performance des mod√®les Gemini."""
    print("üß† Analyse des performances des mod√®les Gemini...")
    try:
        latency_results = prom.custom_query(query=LATENCY_QUERY)
        token_results = prom.custom_query(query=TOKEN_QUERY)

        perf_data = {}
        for result in latency_results:
            model = result['metric']['gen_ai_model']
            latency = float(result['value'][1])
            if model not in perf_data: perf_data[model] = {}
            perf_data[model]['latency_ms'] = latency * 1000

        for result in token_results:
            model = result['metric']['gen_ai_model']
            tokens_per_sec = float(result['value'][1])
            if model not in perf_data: perf_data[model] = {}
            perf_data[model]['tokens_per_sec'] = tokens_per_sec

        print(f"‚úÖ Donn√©es de performance actuelles: {perf_data}")
        return perf_data

    except Exception as e:
        print(f"‚ùå Erreur lors de l'analyse Prometheus: {e}")
        return None

def generate_new_strategy(current_strategy, perf_data):
    """G√©n√®re une nouvelle strat√©gie si une optimisation est trouv√©e."""
    print("ü§î R√©flexion sur une nouvelle strat√©gie...")
    
    # Exemple de logique d'optimisation tr√®s simple :
    # Si 'gemini-1.5-flash' est presque aussi rapide que 'gemini-1.5-pro' mais consomme moins (implicite),
    # on le promeut comme mod√®le par d√©faut.
    
    flash_perf = perf_data.get("gemini-1.5-flash", {})
    pro_perf = perf_data.get("gemini-1.5-pro-002", {})

    if flash_perf and pro_perf:
        if flash_perf.get('latency_ms', 999) < pro_perf.get('latency_ms', 0) * 1.2: # Si flash est max 20% plus lent
            new_default_models = ["gemini-1.5-flash", "gemini-1.5-pro-002"]
            
            if new_default_models != current_strategy['strategy']['default_models']:
                print("üí° NOUVELLE HYPOTH√àSE TROUV√âE: Promouvoir 'gemini-1.5-flash' comme mod√®le par d√©faut.")
                current_strategy['strategy']['default_models'] = new_default_models
                current_strategy['version'] += 0.1
                current_strategy['author'] = "AutonomousOptimizer"
                current_strategy['last_updated'] = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
                return current_strategy
    
    print("üëç Strat√©gie actuelle jug√©e optimale. Aucun changement.")
    return None

def trigger_canary_and_commit(new_strategy):
    """D√©clenche un d√©ploiement Canary, et si r√©ussi, commit les changements sur GitHub."""
    print(f" canary pour la strat√©gie v{new_strategy['version']}...")
    
    # √âtape A: Sauvegarder la nouvelle strat√©gie dans un fichier temporaire
    new_strategy_file = "model_strategy.canary.json"
    with open(new_strategy_file, 'w') as f:
        json.dump(new_strategy, f, indent=2)

    # √âtape B: Appeler le service de d√©ploiement pour lancer le Canary
    # Ce service d√©ploierait une nouvelle version de l'app C# qui lit `model_strategy.canary.json`
    # et surveillerait les m√©triques (erreurs, latence).
    # response = requests.post(CANARY_DEPLOY_ENDPOINT, json={"strategy_file": new_strategy_file})
    # is_canary_successful = response.json().get("success")
    is_canary_successful = True # Simulation d'un succ√®s

    if is_canary_successful:
        print("‚úÖ D√©ploiement Canary r√©ussi. L'am√©lioration est valid√©e.")
        
        # √âtape C: Rendre le changement permanent et le commiter sur GitHub
        print("üíæ Application de la nouvelle strat√©gie et commit sur GitHub...")
        os.rename(new_strategy_file, STRATEGY_FILE)
        
        # Utilisation de l'API GitHub pour cr√©er un commit
        # (N√©cessite un GITHUB_TOKEN avec les permissions appropri√©es)
        # ... logique d'appel √† l'API GitHub pour cr√©er un commit et un push ...
        print("‚úÖ COMMIT AUTOMATIS√â: 'feat(ai): Auto-optimisation de la strat√©gie de mod√®le v{new_strategy['version']}'")
        print("   Le pipeline CI/CD va maintenant d√©ployer cette am√©lioration de mani√®re permanente.")

    else:
        print("‚ùå D√©ploiement Canary √©chou√©. Annulation de la nouvelle strat√©gie.")
        os.remove(new_strategy_file)

def main():
    print("ü§ñ D√©marrage de l'Optimiseur de Performance Autonome...")
    prom = PrometheusConnect(url=PROMETHEUS_URL, disable_ssl=True)
    
    while True:
        with open(STRATEGY_FILE, 'r') as f:
            current_strategy = json.load(f)
        
        perf_data = analyze_performance(prom)
        if perf_data:
            new_strategy = generate_new_strategy(dict(current_strategy), perf_data)
            if new_strategy:
                trigger_canary_and_commit(new_strategy)
        
        print("üò¥ Attente de 1 heure avant le prochain cycle d'optimisation...")
        time.sleep(3600)

if __name__ == "__main__":
    main()