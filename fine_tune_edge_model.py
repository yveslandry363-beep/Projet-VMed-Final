# Fichier: fine_tune_edge_model.py
# Description: Script pour fine-tuner un mod√®le l√©ger (Gemma) pour le d√©ploiement Edge.

import torch
from datasets import load_dataset
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer,
)

# --- CONFIGURATION ---
MODEL_NAME = "google/gemma-2b" # Mod√®le l√©ger et performant de Google
DATASET_PATH = "path/to/your/medical_dataset.csv" # CSV avec colonnes "diagnostic_text", "ia_guidance"
OUTPUT_DIR = "./results_edge_model"
FINE_TUNED_MODEL_DIR = "./fine_tuned_gemma_medical"

def main():
    print(f"üöÄ D√©marrage du fine-tuning du mod√®le Edge: {MODEL_NAME}")

    # 1. Charger le jeu de donn√©es (doit √™tre au format question/r√©ponse)
    print(f"üíæ Chargement du jeu de donn√©es depuis {DATASET_PATH}...")
    # On formate le dataset pour l'entra√Ænement
    def format_dataset(example):
        return {"text": f"### Diagnostic:\n{example['diagnostic_text']}\n\n### Recommandation:\n{example['ia_guidance']}"}

    dataset = load_dataset("csv", data_files=DATASET_PATH).map(format_dataset)
    print("‚úÖ Jeu de donn√©es charg√© et format√©.")

    # 2. Configuration de la quantification pour r√©duire l'usage m√©moire (QLoRA)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.bfloat16,
        bnb_4bit_use_double_quant=False,
    )

    # 3. Charger le mod√®le et le tokenizer
    print("üß† Chargement du mod√®le de base et du tokenizer...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        quantization_config=bnb_config,
        device_map="auto" # Utilise le GPU si disponible
    )
    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)
    tokenizer.pad_token = tokenizer.eos_token
    print("‚úÖ Mod√®le et tokenizer charg√©s.")

    # 4. D√©finir les arguments d'entra√Ænement
    training_arguments = TrainingArguments(
        output_dir=OUTPUT_DIR,
        num_train_epochs=1, # 1 √† 3 √©poques suffisent souvent pour le fine-tuning
        per_device_train_batch_size=4,
        gradient_accumulation_steps=1,
        learning_rate=2e-4,
        fp16=True,
        logging_steps=25,
    )

    # 5. Cr√©er et lancer l'entra√Æneur
    print("üèÉ‚Äç‚ôÇÔ∏è D√©marrage de l'entra√Ænement...")
    trainer = Trainer(
        model=model,
        train_dataset=dataset['train'],
        args=training_arguments,
        data_collator=lambda data: {'input_ids': torch.stack([f['input_ids'] for f in data]),
                                     'attention_mask': torch.stack([f['attention_mask'] for f in data]),
                                     'labels': torch.stack([f['input_ids'] for f in data])}
    )
    trainer.train()
    print("‚úÖ Entra√Ænement termin√©.")

    # 6. Sauvegarder le mod√®le fine-tun√© pour le d√©ploiement Edge
    print(f"üíæ Sauvegarde du mod√®le fine-tun√© dans '{FINE_TUNED_MODEL_DIR}'...")
    trainer.save_model(FINE_TUNED_MODEL_DIR)
    print("üèÅ Mod√®le Edge pr√™t √† √™tre d√©ploy√© !")

if __name__ == "__main__":
    main()