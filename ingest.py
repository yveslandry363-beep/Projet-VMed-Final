# Fichier: ingest.py
# Description: Service d'ingestion pour la base de connaissances RAG.
#              Scanne les PDF, les d√©coupe, les vectorise et les stocke dans Milvus.

import os
from dotenv import load_dotenv
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import Milvus

load_dotenv()

# --- CONFIGURATION ---
PDF_SOURCE_DIR = "recherche_medicale"
MILVUS_HOST = "milvus" # Utilise le nom du service Docker
MILVUS_PORT = "19530"
COLLECTION_NAME = "medical_knowledge_base"

# Mod√®le d'embedding de Google (transforme le texte en vecteurs)
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

def main():
    """
    Point d'entr√©e du script d'ingestion.
    """
    print("üöÄ D√©marrage du service d'ingestion RAG...")

    # 1. Charger les documents PDF depuis le dossier
    print(f"üìÑ √âtape 1/4: Chargement des documents depuis '{PDF_SOURCE_DIR}'...")
    if not os.path.exists(PDF_SOURCE_DIR) or not os.listdir(PDF_SOURCE_DIR):
        print(f"‚ùå ERREUR: Le dossier '{PDF_SOURCE_DIR}' est vide ou n'existe pas.")
        print("Veuillez y placer vos fichiers PDF de recherche m√©dicale.")
        return

    loader = PyPDFDirectoryLoader(PDF_SOURCE_DIR)
    docs = loader.load()
    print(f"‚úÖ {len(docs)} documents charg√©s.")

    # 2. D√©couper les documents en morceaux (chunks)
    print("üî™ √âtape 2/4: D√©coupage des documents en morceaux (chunks)...")
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000, 
        chunk_overlap=150
    )
    chunks = text_splitter.split_documents(docs)
    print(f"‚úÖ {len(chunks)} morceaux de texte cr√©√©s.")

    # 3. Vectoriser et stocker dans Milvus
    print("üß† √âtape 3/4: Vectorisation et stockage dans Milvus...")
    print(f"   (Connexion √† Milvus sur {MILVUS_HOST}:{MILVUS_PORT})")
    
    try:
        # --- AM√âLIORATION "JAMAIS VUE": INGESTION PAR BATCHS ---
        # LangChain g√®re automatiquement l'envoi par batchs √† l'API d'embedding,
        # ce qui est beaucoup plus rapide que d'envoyer les chunks un par un.
        vector_store = Milvus.from_documents(
            documents=chunks,
            embedding=embeddings,
            collection_name=COLLECTION_NAME,
            connection_args={"host": MILVUS_HOST, "port": MILVUS_PORT},
            batch_size=128 # Envoi de 128 chunks √† la fois pour vectorisation
        )
        print("‚úÖ Base de connaissances vectorielle cr√©√©e/mise √† jour avec succ√®s.")
    except Exception as e:
        print(f"‚ùå ERREUR lors de la connexion ou de l'ingestion dans Milvus: {e}")
        print("   Assurez-vous que votre stack Docker (Milvus, etcd, MinIO) est bien d√©marr√©e.")
        return

    # 4. V√©rification
    print("üîç √âtape 4/4: V√©rification rapide...")
    try:
        retriever = vector_store.as_retriever(search_kwargs={'k': 1})
        test_query = "sympt√¥mes de la grippe"
        result = retriever.invoke(test_query)
        if result:
            print(f"‚úÖ Test de recherche r√©ussi. Un document similaire √† '{test_query}' a √©t√© trouv√©.")
            print("--- Extrait ---")
            print(result[0].page_content[:200] + "...")
            print("---------------")
        else:
            print("‚ö†Ô∏è Test de recherche n'a retourn√© aucun r√©sultat.")
    except Exception as e:
        print(f"‚ùå ERREUR lors du test de recherche: {e}")

    print("\nüèÅ Ingestion termin√©e.")

if __name__ == "__main__":
    main()